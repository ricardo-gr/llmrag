{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757c603d",
   "metadata": {},
   "source": [
    "# Basic RAG test\n",
    "\n",
    "## Environment Setup\n",
    "This section sets up environment variables for API endpoints and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff1d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['LLM_ENDPOINT'] = os.getenv('LLM_ENDPOINT')\n",
    "\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be6c36",
   "metadata": {},
   "source": [
    "## Load and Parse Web Page\n",
    "This section loads the web page and parses its content using BeautifulSoup and LangChain's WebBaseLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae3696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd07b5e",
   "metadata": {},
   "source": [
    "## Split Documents into Chunks\n",
    "This section splits the loaded documents into manageable text chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e96df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722092b1",
   "metadata": {},
   "source": [
    "## Create Vector Store and Generate Embeddings\n",
    "This section creates a Chroma vector store and generates embeddings for the text chunks using Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d83648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=\"qwen3:14b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b22b00",
   "metadata": {},
   "source": [
    "## Cleanup Vector Store\n",
    "This section deletes the Chroma vector store collection to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04187181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c59679",
   "metadata": {},
   "source": [
    "## Retrieve and Print Document\n",
    "This section retrieves a document relevant to the query and prints its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189ea251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14e975",
   "metadata": {},
   "source": [
    "## Retrieve Prompt from LangChain Hub\n",
    "This section pulls a prompt from the LangChain hub and prints it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05128690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364714b",
   "metadata": {},
   "source": [
    "## Initialize Ollama LLM\n",
    "This section initializes the Ollama language model (LLM) with the specified model and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d439224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3:14b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e098b03",
   "metadata": {},
   "source": [
    "## Build RAG Pipeline\n",
    "This section defines the output parser, document formatting function, and constructs the Retrieval-Augmented Generation (RAG) chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497da2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "     | prompt\n",
    "     | llm\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567b42d",
   "metadata": {},
   "source": [
    "## Run RAG Chain and Display Response\n",
    "This section invokes the RAG chain with a sample question and prints the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04ab0b",
   "metadata": {},
   "source": [
    "# Multi_Query Generation test\n",
    "\n",
    "## Prompt Generation and Chain\n",
    "\n",
    "A prompt template is created and a chain for alternate query generation is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question} /no_think\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOllama(\n",
    "        model=\"qwen3:14B\",\n",
    "        temperature=0\n",
    "    )\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f14698",
   "metadata": {},
   "source": [
    "## Generate and Clean Alternative Queries\n",
    "\n",
    "This section generates multiple alternative versions of a user question using an LLM, cleans the output to remove any extraneous tokens, and displays the resulting queries. These alternative queries help improve document retrieval by providing diverse perspectives for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4157a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: What is the process of breaking down complex tasks into smaller subtasks for LLM agents?  \n",
      "Query 2: How do LLM agents utilize task decomposition to improve performance on complex problems?  \n",
      "Query 3: Can you explain the concept of task decomposition in the context of large language model agents?  \n",
      "Query 4: What role does task decomposition play in enabling LLM agents to handle multi-step tasks?  \n",
      "Query 5: In what ways is task decomposition applied to enhance the coordination of LLM agents in collaborative settings?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Task Decomposition for LLM agents?\"\n",
    "generated_queries_list = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "def clean_llm_output(queries):\n",
    "    first = 0\n",
    "    last = 0\n",
    "    for i, q in enumerate(queries):\n",
    "        if \"<think>\" in q:\n",
    "            first = i\n",
    "        if first >= 0 and \"</think>\" in q:\n",
    "            last = i+2\n",
    "\n",
    "    return queries[last:]\n",
    "\n",
    "cleaned_generated_queries_list = clean_llm_output(generated_queries_list)\n",
    "\n",
    "for i, q in enumerate(cleaned_generated_queries_list):\n",
    "    print(f\"Query {i+1}: {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcca31",
   "metadata": {},
   "source": [
    "## Unique Document Retrieval Chain\n",
    "\n",
    "This section defines a retrieval chain that generates multiple alternative queries for a user question, retrieves relevant documents for each query, and returns the unique union of all retrieved documents. This approach improves retrieval diversity and reduces redundancy by combining results from different query perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "677720cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8 unique documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" A simple function to get the unique union of retrieved documents \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "retrieval_chain = generate_queries | clean_llm_output | retriever.map() | get_unique_union\n",
    "\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "print(f\"Retrieved {len(docs)} unique documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d5e69",
   "metadata": {},
   "source": [
    "## Multi-Query RAG Chain Execution\n",
    "\n",
    "This section constructs and executes a Retrieval-Augmented Generation (RAG) chain that leverages multiple alternative queries for improved document retrieval. It combines the results from diverse query perspectives and generates a comprehensive answer to the user question based on the aggregated context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "faf252a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to figure out what Task Decomposition means for LLM agents based on the provided context. Let me start by going through the documents again to pick up relevant information.\n",
      "\n",
      "The first document mentions that the AI assistant can parse user input into several tasks, each with attributes like task, ID, dependencies, and arguments. The \"dep\" field indicates dependencies on previous tasks. It also says tasks must be selected from an available list and that there's a logical order. If parsing isn't possible, return empty JSON. This seems to relate to breaking down a user's request into multiple tasks with dependencies.\n",
      "\n",
      "Another document talks about HuggingGPT's four stages, with the first being Task planning where an LLM acts as the brain to parse user requests into multiple tasks. Each task has attributes: type, ID, dependencies, and arguments. They use few-shot examples for task parsing. This sounds like task decomposition, breaking down the user's input into structured tasks.\n",
      "\n",
      "The overview of an LLM-powered autonomous agent system mentions memory and tool use, but maybe not directly task decomposition. However, the part about API-Bank and the benchmark levels (Level-1 to Level-3) discusses tool use capabilities, which might be part of task decomposition when planning API calls.\n",
      "\n",
      "The example with Mario and the platform game might be a demonstration of task decomposition, but the context here is more about code writing after clarifications, so maybe not directly relevant.\n",
      "\n",
      "The Chain of Hindsight (CoH) is about improving outputs with feedback, which seems more about refinement than decomposition.\n",
      "\n",
      "Putting this together, Task Decomposition for LLM agents involves breaking down user requests into multiple structured tasks with specific attributes (type, ID, dependencies, arguments). The LLM parses the input, identifies dependencies between tasks, and ensures they follow a logical order. This is part of the task planning stage, as seen in HuggingGPT's approach, where the LLM uses examples to guide the decomposition. The decomposition allows the system to handle complex requests by managing individual tasks and their dependencies, possibly involving external tools or APIs as needed.\n",
      "</think>\n",
      "\n",
      "Task Decomposition for LLM agents involves breaking down user requests into structured, interdependent tasks with defined attributes. Specifically, the LLM parses the input into multiple tasks, each characterized by:  \n",
      "1. **Task Type**: Selected from a predefined list (e.g., text generation, image processing).  \n",
      "2. **ID**: A unique identifier for tracking and referencing tasks.  \n",
      "3. **Dependencies (dep)**: Logical relationships to prior tasks that generate necessary resources (e.g., using output from a previous task as input for the next).  \n",
      "4. **Arguments**: Inputs like text, images, audio, or video URLs required for task execution.  \n",
      "\n",
      "This process is critical in systems like **HuggingGPT**, where the LLM acts as a \"task planner\" to decompose complex user queries into sequential steps, ensuring proper ordering and resource sharing. Dependencies are managed via the `dep` field, which links tasks to their prerequisites (e.g., `-task_id` references outputs from prior tasks). The decomposition is guided by few-shot examples and logical constraints, enabling the agent to handle multi-step workflows, such as combining API calls or integrating external tools (e.g., search engines, calculators) as needed.  \n",
      "\n",
      "In essence, task decomposition allows LLM agents to transform high-level user intentions into executable, modular steps, leveraging both internal reasoning and external toolkits for complex problem-solving.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "multi_query_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "mqrc_output = multi_query_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "print(mqrc_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
