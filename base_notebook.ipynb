{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757c603d",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "This section sets up environment variables for API endpoints and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff1d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['LLM_ENDPOINT'] = os.getenv('LLM_ENDPOINT')\n",
    "\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be6c36",
   "metadata": {},
   "source": [
    "# Load and Parse Web Page\n",
    "This section loads the web page and parses its content using BeautifulSoup and LangChain's WebBaseLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae3696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd07b5e",
   "metadata": {},
   "source": [
    "# Split Documents into Chunks\n",
    "This section splits the loaded documents into manageable text chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e96df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722092b1",
   "metadata": {},
   "source": [
    "# Create Vector Store and Generate Embeddings\n",
    "This section creates a Chroma vector store and generates embeddings for the text chunks using Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d83648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=\"qwen3:14b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b22b00",
   "metadata": {},
   "source": [
    "# Cleanup Vector Store\n",
    "This section deletes the Chroma vector store collection to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04187181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c59679",
   "metadata": {},
   "source": [
    "# Retrieve and Print Document\n",
    "This section retrieves a document relevant to the query and prints its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189ea251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea14e975",
   "metadata": {},
   "source": [
    "# Retrieve Prompt from LangChain Hub\n",
    "This section pulls a prompt from the LangChain hub and prints it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05128690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364714b",
   "metadata": {},
   "source": [
    "# Initialize Ollama LLM\n",
    "This section initializes the Ollama language model (LLM) with the specified model and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d439224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3:14b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e098b03",
   "metadata": {},
   "source": [
    "# Build RAG Pipeline\n",
    "This section defines the output parser, document formatting function, and constructs the Retrieval-Augmented Generation (RAG) chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497da2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "     | prompt\n",
    "     | llm\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567b42d",
   "metadata": {},
   "source": [
    "# Run RAG Chain and Display Response\n",
    "This section invokes the RAG chain with a sample question and prints the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba5e1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about Task Decomposition. Let me check the context provided.\n",
      "\n",
      "The first part of the context mentions that task decomposition can be done by LLM with simple prompting, using task-specific instructions, or with human inputs. Then there's another approach called LLM+P which uses an external planner and PDDL. The answer should cover these methods briefly.\n",
      "\n",
      "I need to make sure I don't include the other sections about risks, generative agents, or APIs since they aren't related to task decomposition. The answer should be concise, three sentences max. Let me structure it: define task decomposition, mention the methods (LLM prompting, task-specific instructions, human input), and then the LLM+P approach with PDDL. That should cover it without going over the limit.\n",
      "</think>\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, manageable subtasks. It can be achieved through LLM prompting, task-specific instructions, or human input. An alternative approach uses external planners with PDDL to handle long-horizon planning.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
